# estimate the pose of target objects detected
import numpy as np
import json
import os
import ast
import cv2
import sys
from ultralytics import YOLO    
from network.scripts.detector import Detector
from scipy.signal import medfilt

# list of target fruits and vegs types
# Make sure the names are the same as the ones used in your YOLO model
TARGET_TYPES = ['orange','red apple', 'capsicum','green apple','mango']


def estimate_pose(camera_matrix, obj_info, robot_pose):
    """
    function:
        estimate the pose of a target based on size and location of its bounding box and the corresponding robot pose
    input:
        camera_matrix: list, the intrinsic matrix computed from camera calibration (read from 'param/intrinsic.txt')
            |f_x, s,   c_x|
            |0,   f_y, c_y|
            |0,   0,   1  |
            (f_x, f_y): focal length in pixels
            (c_x, c_y): optical centre in pixels
            s: skew coefficient (should be 0 for PenguinPi)
        obj_info: list, an individual bounding box in an image (generated by get_bounding_box, [label,[x,y,width,height]])
        robot_pose: list, pose of robot corresponding to the image (read from 'lab_output/images.txt', [x,y,theta])
    output:
        target_pose: dict, prediction of target pose
    """
    # read in camera matrix (from camera calibration results)
    focal_length = camera_matrix[0][0]

    # there are 8 possible types of fruits and vegs
    ######### Replace with your codes #########
    # TODO: measure actual sizes of targets [width, depth, height] and update the dictionary of true target dimensions
    target_dimensions_dict = {'orange': [0.075, 0.075, 0.072], 'red apple':  [0.074, 0.074, 0.087], 
                              'mango':  [0.113, 0.067, 0.058], 'green apple': [0.081, 0.081, 0.067], 
                              'capsicum': [0.073, 0.073, 0.088]}
    #########

    # estimate target pose using bounding box and robot pose
    target_class = obj_info[0]     # get predicted target label of the box
    target_box = obj_info[1]       # get bounding box measures: [x,y,width,height]
    true_height = target_dimensions_dict[target_class][2]   # look up true height of by class label

    # compute pose of the target based on bounding box info, true object height, and robot's pose
    pixel_height = target_box[3]
    pixel_center = target_box[0]
    distance = true_height/pixel_height * focal_length  # estimated distance between the robot and the centre of the image plane based on height
    # training image size 320x240p
    image_width = 640 # change this if your training image is in a different size (check details of pred_0.png taken by your robot)
    x_shift = image_width/2 - pixel_center              # x distance between bounding box centre and centreline in camera view
    theta = np.arctan(x_shift/focal_length)     # angle of object relative to the robot
    ang = theta + robot_pose[2]     # angle of object in the world frame
    
   # relative object location
    distance_obj = distance/np.cos(theta) # relative distance between robot and object
    x_relative = distance_obj * np.cos(theta) # relative x pose
    y_relative = distance_obj * np.sin(theta) # relative y pose
    relative_pose = {'x': x_relative, 'y': y_relative}
    #print(f'relative_pose: {relative_pose}')

    # location of object in the world frame using rotation matrix
    delta_x_world = x_relative * np.cos(ang) - y_relative * np.sin(ang)
    delta_y_world = x_relative * np.sin(ang) + y_relative * np.cos(ang)
    # add robot pose with delta target pose
    target_pose = {'y': (robot_pose[1]+delta_y_world)[0],
                   'x': (robot_pose[0]+delta_x_world)[0]}
    #print(f'delta_x_world: {delta_x_world}, delta_y_world: {delta_y_world}')
    #print(f'target_pose: {target_pose}')

    return target_pose


def merge_estimations(target_pose_dict):
    num_per_target = 1
    """
    function:
        merge estimations of the same target
    input:
        target_pose_dict: dict, generated by estimate_pose
    output:
        target_est: dict, target pose estimations after merging
    """
    target_est = {}

    ######### Replace with your codes #########
    # TODO: replace it with a solution to merge the multiple occurrences of the same class type (e.g., by a distance threshold)
    target_est = target_pose_dict

    redapple_est = []
    greenapple_est = []
    orange_est = []
    mango_est = []
    capsicum_est = []

    for est in target_pose_dict:
        if est.__contains__("   red apple"):
            redapple_est.append([target_pose_dict[est]["x"], target_pose_dict[est]["y"]])
        elif est.__contains__("green apple"):
            greenapple_est.append([target_pose_dict[est]["x"], target_pose_dict[est]["y"]])
        elif est.__contains__("mango"):
            mango_est.append([target_pose_dict[est]["x"], target_pose_dict[est]["y"]])
        elif est.__contains__("capsicum"):
            capsicum_est.append([target_pose_dict[est]["x"], target_pose_dict[est]["y"]])
        else:
            orange_est.append([target_pose_dict[est]["x"], target_pose_dict[est]["y"]])


    if len(redapple_est) > num_per_target:
        filtered = []
        xcoords = np.array(redapple_est)[:, 0]
        ycoords = np.array(redapple_est)[:, 1]
        q75_x, q25_x = np.percentile(xcoords,[75,25])
        q75_y,q25_y = np.percentile(ycoords,[75,25])
        range_x = [q25_x-1.5*(q75_x-q25_x), q75_x+1.5*(q75_x-q25_x)]
        range_y = [q25_y-(1.5*q75_y-q25_y) ,q75_y+(1.5*q75_y-q25_y)]
        for i in range(len(redapple_est)):
            if (redapple_est[i][0] < range_x[1] and redapple_est[i][0] > range_x[0]):
                if (redapple_est[i][1] < range_y[1] and redapple_est[i][1] > range_y[0]):
                    filtered.append(redapple_est[i])
        redapple_est = np.average(filtered,axis=0)  


    if len(greenapple_est) > num_per_target:
        filtered = []
        xcoords = np.array(greenapple_est)[:, 0]
        ycoords = np.array(greenapple_est)[:, 1]
        q75_x, q25_x = np.percentile(xcoords,[75,25])
        range_x = [q25_x-1.5*(q75_x-q25_x), q75_x+1.5*(q75_x-q25_x)] # min max
        q75_y,q25_y = np.percentile(ycoords,[75,25])
        range_y = [q25_y-(1.5*q75_y-q25_y) ,q75_y+(1.5*q75_y-q25_y)] # min max
        for i in range(len(greenapple_est)):
            if (greenapple_est[i][0] < range_x[1] and greenapple_est[i][0] > range_x[0]):
                if (greenapple_est[i][1] < range_y[1] and greenapple_est[i][1] > range_y[0]):
                    filtered.append(greenapple_est[i])
        greenapple_est = np.average(filtered,axis=0)  

    if len(orange_est) > num_per_target:
        filtered = []
        xcoords = np.array(orange_est)[:, 0]
        ycoords = np.array(orange_est)[:, 1]
        q75_x, q25_x = np.percentile(xcoords,[75,25])
        range_x = [q25_x-1.5*(q75_x-q25_x), q75_x+1.5*(q75_x-q25_x)] # min max
        q75_y,q25_y = np.percentile(ycoords,[75,25])
        range_y = [q25_y-(1.5*q75_y-q25_y) ,q75_y+(1.5*q75_y-q25_y)] # min max
        for i in range(len(orange_est)):
            if (orange_est[i][0] < range_x[1] and orange_est[i][0] > range_x[0]):
                if (orange_est[i][1] < range_y[1] and orange_est[i][1] > range_y[0]):
                    filtered.append(orange_est[i])
        orange_est = np.average(filtered,axis=0) 

    if len(mango_est) > num_per_target:
        filtered = []
        xcoords = np.array(mango_est)[:, 0]
        ycoords = np.array(mango_est)[:, 1]
        q75_x, q25_x = np.percentile(xcoords,[75,25])
        range_x = [q25_x-1.5*(q75_x-q25_x), q75_x+1.5*(q75_x-q25_x)] # min max
        q75_y,q25_y = np.percentile(ycoords,[75,25])
        range_y = [q25_y-(1.5*q75_y-q25_y) ,q75_y+(1.5*q75_y-q25_y)] # min max
        for i in range(len(mango_est)):
            if (mango_est[i][0] < range_x[1] and mango_est[i][0] > range_x[0]):
                if (mango_est[i][1] < range_y[1] and mango_est[i][1] > range_y[0]):
                    filtered.append(mango_est[i])
        mango_est = np.average(filtered,axis=0)

    if len(capsicum_est) > num_per_target:
        filtered = []
        xcoords = np.array(capsicum_est)[:, 0]
        ycoords = np.array(capsicum_est)[:, 1]
        q75_x, q25_x = np.percentile(xcoords,[75,25])
        range_x = [q25_x-1.5*(q75_x-q25_x), q75_x+1.5*(q75_x-q25_x)] # min max
        q75_y,q25_y = np.percentile(ycoords,[75,25])
        range_y = [q25_y-(1.5*q75_y-q25_y) ,q75_y+(1.5*q75_y-q25_y)] # min max
        for i in range(len(capsicum_est)):
            if (capsicum_est[i][0] < range_x[1] and capsicum_est[i][0] > range_x[0]):
                if (capsicum_est[i][1] < range_y[1] and capsicum_est[i][1] > range_y[0]):
                    filtered.append(capsicum_est[i])
        capsicum_est = np.average(filtered,axis=0)

    target_est = target_pose_dict
    target_est = {
        "red apple" : {
            "x":redapple_est[0],
            "y":redapple_est[1]
        },
        "green apple": {
            "x":greenapple_est[0],
            "y":greenapple_est[1]
        },
        "orange": {
            "x":orange_est[0],
            "y":orange_est[1]
        },
        "mango": {
            "x":mango_est[0],
            "y":mango_est[1] 
        },
        "capsicum":{
            "x":capsicum_est[0],
            "y":capsicum_est[1]
        }
    }
    
    return target_est


# main loop
if __name__ == "__main__":
    script_dir = os.path.dirname(os.path.abspath(__file__))     # get current script directory (TargetPoseEst.py)

    # read in camera matrix
    fileK = f'{script_dir}/calibration/param/intrinsic.txt'
    camera_matrix = np.loadtxt(fileK, delimiter=',')

    # init YOLO model
    model_path = f'{script_dir}/network/scripts/model/yolov8_model.pt'
    yolo = Detector(model_path)

    # create a dictionary of all the saved images with their corresponding robot pose
    image_poses = {}
    with open(f'{script_dir}/lab_output/images.txt') as fp:
        for line in fp.readlines():
            pose_dict = ast.literal_eval(line)
            image_poses[pose_dict['imgfname']] = pose_dict['pose']

    # estimate pose of targets in each image
    target_pose_dict = {}
    detected_type_list = []
    for image_path in image_poses.keys():
        input_image = cv2.imread(image_path)
        bounding_boxes, bbox_img = yolo.detect_single_image(input_image)
        # cv2.imshow('bbox', bbox_img)
        # cv2.waitKey(0)
        robot_pose = image_poses[image_path]

        for detection in bounding_boxes:
            # count the occurrence of each target type
            occurrence = detected_type_list.count(detection[0])
            target_pose_dict[f'{detection[0]}_{occurrence}'] = estimate_pose(camera_matrix, detection, robot_pose)

            detected_type_list.append(detection[0])

    # merge the estimations of the targets so that there are at most 3 estimations of each target type
    target_est = {}
    target_est = merge_estimations(target_pose_dict)
    print(target_est)
    # save target pose estimations
    with open(f'{script_dir}/lab_output/targets.txt', 'w') as fo:
        json.dump(target_est, fo, indent=4)

    print('Estimations saved!')